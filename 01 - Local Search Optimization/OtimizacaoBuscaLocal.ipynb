{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jtXqfP_3WaR"
   },
   "source": [
    "**Instituto Tecnológico de Aeronáutica – ITA**\n",
    "\n",
    "**Inteligência Artificial PEE 2022**\n",
    "\n",
    "**Faculty Advisor:**\n",
    "\n",
    "Marcos Ricardo Omena de Albuquerque Maximo\n",
    "\n",
    "# Otimização com Métodos de Busca Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe6tAz_k3vJn"
   },
   "source": [
    "# Descrição do problema\n",
    "\n",
    "O problema ser resolvido é a otimização de funções matemáticas (em que é possível obter amostras da função e de sua derivada) usando os métodos descritos, assim sua implementação não deve ser específica para o caso de teste. As descrições dos algoritmos que serão implementados podem ser vistas nos slides do curso.\n",
    "\n",
    "No caso de teste, o problema específico a ser resolvido é a determinação do coeficiente de desaceleração de uma bola em movimento num campo de futebol de robôs. A bola em movimento perde energia devido a um fenômeno conhecido como *rolling friction*. Conforme explicado em aula, pode-se determinar o coeficiente de desaceleração através do seguinte algoritmo:\n",
    "\n",
    "1. Usar câmera e visão computacional para obter posições $(x,y)$ da bola em cada instante.\n",
    "2. Calcular velocidades em $x$ e $y$ usando diferenças finitas centradas (exceto no primeiro e último elementos, em que deve-se usar derivadas *forward* e *backward*, respectivamente):\n",
    "\n",
    "    $$v_x [k]=(x[k+1]-x[k-1])/(t[k+1]-t[k-1])$$\n",
    "    $$v_y [k]=(y[k+1]-y[k-1])/(t[k+1]-t[k-1])$$\n",
    "\n",
    "3. Calcular $v[k]= \\sqrt{v_x^2[k] + v_y^2[k]}$.\n",
    "4. Obter $v_0$ e $f$ através de uma otimização com função de custo:\n",
    "\n",
    "    $$J([v_0,f])= \\sum_{k=1}^n (v_0 + f t[k]-v[k])^2$$\n",
    "    \n",
    "Desse modo, tem-se os seguintes parâmetros a serem otimizados: \n",
    "\n",
    "$\\theta_0 = v_0$ e $\\theta_1 = f \\Rightarrow \\boldsymbol{\\theta} =[v_0  \\: f]^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEULvhCFLTmO"
   },
   "source": [
    "# Carregamento do arquivo de dados\n",
    "\n",
    "Rode a célula a seguir e carregue o arquivo data.txt contendo dados reais capturados com uma câmera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJVOY32doWx4"
   },
   "outputs": [],
   "source": [
    "# This is removed, used only in Collaboratory.\n",
    "## from google.colab import files\n",
    "## uploaded = files.upload()\n",
    "\n",
    "# Define the filepath for the dataset\n",
    "DATASET_FILE = \"data.txt\"\n",
    "\n",
    "# Read the dataset file\n",
    "dataset_file = open(DATASET_FILE, \"r\")\n",
    "dataset = dataset_file.readlines()\n",
    "dataset_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg5SzS5aLuWP"
   },
   "source": [
    "# Algoritmos já implementados!\n",
    "\n",
    "Sugiro que leia as implementações a seguir para melhor entender como as interfaces dos algoritmos funcionam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqhxWonPnVHE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def least_squares(phi, x, y):\n",
    "    \"\"\"\n",
    "    Executes the Least Squares Method in order to fit a model to a collection of data points (x, y).\n",
    "\n",
    "    :param phi: functions of the model.\n",
    "    :type phi: list of functions.\n",
    "    :param x: independent variable.\n",
    "    :type x: numpy.array of float.\n",
    "    :param y: dependent variable.\n",
    "    :type y: numpy.array of float.\n",
    "    :return: least square estimate of the model parameters.\n",
    "    :rtype: numpy.array of float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of data points\n",
    "    m = len(x)\n",
    "\n",
    "    # Number of model functions\n",
    "    n = len(phi) - 1\n",
    "\n",
    "    # Initializing solution vectors\n",
    "    a = np.zeros((n + 1, n + 1))\n",
    "    b = np.zeros((n + 1))\n",
    "\n",
    "    # Making matrix A\n",
    "    for k in range(n + 1):\n",
    "        for j in range(n + 1):\n",
    "            for i in range(m):\n",
    "                a[k, j] += phi[k](x[i]) * phi[j](x[i])\n",
    "\n",
    "    # Making vector b\n",
    "    for j in range(n + 1):\n",
    "        for i in range(m):\n",
    "            b[j] += phi[j](x[i]) * y[i]\n",
    "\n",
    "    # Solving system A * x = b\n",
    "    theta = np.linalg.solve(a, b)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldedWGqVndYB"
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "import random\n",
    "\n",
    "\n",
    "def simulated_annealing(\n",
    "    cost_function, random_neighbor, schedule, theta0, epsilon, max_iterations\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes the Simulated Annealing (SA) algorithm to minimize (optimize) a cost function.\n",
    "\n",
    "    :param cost_function: function to be minimized.\n",
    "    :type cost_function: function.\n",
    "    :param random_neighbor: function which returns a random neighbor of a given point.\n",
    "    :type random_neighbor: numpy.array.\n",
    "    :param schedule: function which computes the temperature schedule.\n",
    "    :type schedule: function.\n",
    "    :param theta0: initial guess.\n",
    "    :type theta0: numpy.array.\n",
    "    :param epsilon: used to stop the optimization if the current cost is less than epsilon.\n",
    "    :type epsilon: float.\n",
    "    :param max_iterations: maximum number of iterations.\n",
    "    :type max_iterations: int.\n",
    "    :return theta: local minimum.\n",
    "    :rtype theta: np.array.\n",
    "    :return history: history of points visited by the algorithm.\n",
    "    :rtype history: list of np.array.\n",
    "    \"\"\"\n",
    "    theta = theta0\n",
    "    history = []\n",
    "    for i in range(max_iterations):\n",
    "        history.append(theta)\n",
    "        current_cost = cost_function(theta)\n",
    "        if current_cost < epsilon:\n",
    "            return theta, history\n",
    "        t = schedule(i)\n",
    "        if t <= 0.0:\n",
    "            return theta, history\n",
    "        neighbor = random_neighbor(theta)\n",
    "        deltae = cost_function(neighbor) - current_cost\n",
    "        if deltae < 0.0:\n",
    "            theta = neighbor\n",
    "        else:\n",
    "            # Cooldown process\n",
    "            r = random.uniform(0.0, 1.0)\n",
    "            if r <= exp(-deltae / t):\n",
    "                theta = neighbor\n",
    "    return theta, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvApqLzjL6Lk"
   },
   "source": [
    "# Algoritmos para implementar\n",
    "\n",
    "Dicas:\n",
    "- As implementações são similares aos pseudocódigos mostrados em aula.\n",
    "- Como critério de parada, verifique se o custo atual é menor que um epsilon, i.e. verifique se $J(\\boldsymbol{\\theta})<\\varepsilon$.\n",
    "- As implementações devem ser genéricas e não depende do problema em específico que estamos resolvido neste exercício.\n",
    "- A variável `history` deve ser usada para guardar as posições que o algoritmo visita durante a otimização.\n",
    "- Leia as documentações das funções para entender quais são as entradas e as saídas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftJmALzYm72N"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    cost_function, gradient_function, theta0, alpha, epsilon, max_iterations\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes the Gradient Descent (GD) algorithm to minimize (optimize) a cost function.\n",
    "\n",
    "    :param cost_function: function to be minimized.\n",
    "    :type cost_function: function.\n",
    "    :param gradient_function: gradient of the cost function.\n",
    "    :type gradient_function: function.\n",
    "    :param theta0: initial guess.\n",
    "    :type theta0: numpy.array.\n",
    "    :param alpha: learning rate.\n",
    "    :type alpha: float.\n",
    "    :param epsilon: used to stop the optimization if the current cost is less than epsilon.\n",
    "    :type epsilon: float.\n",
    "    :param max_iterations: maximum number of iterations.\n",
    "    :type max_iterations: int.\n",
    "    :return theta: local minimum.\n",
    "    :rtype theta: numpy.array.\n",
    "    :return history: history of points visited by the algorithm.\n",
    "    :rtype history: list of numpy.array.\n",
    "    \"\"\"\n",
    "    theta = theta0\n",
    "    history = [theta0]\n",
    "\n",
    "    # Stopping criteria\n",
    "    while cost_function(theta) > epsilon:\n",
    "\n",
    "        # Update value for theta\n",
    "        theta -= alpha * gradient_function(theta)\n",
    "\n",
    "        print(\n",
    "            f\"Theta: {theta} | Cost: {cost_function(theta)} | Gradient: {gradient_function(theta)}\"\n",
    "        )\n",
    "\n",
    "        # Save new theta to history\n",
    "        history.append(theta)\n",
    "\n",
    "    return theta, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cibVipgnR3I"
   },
   "outputs": [],
   "source": [
    "from math import inf\n",
    "\n",
    "\n",
    "def hill_climbing(cost_function, neighbors, theta0, epsilon, max_iterations):\n",
    "    \"\"\"\n",
    "    Executes the Hill Climbing (HC) algorithm to minimize (optimize) a cost function.\n",
    "\n",
    "    :param cost_function: function to be minimized.\n",
    "    :type cost_function: function.\n",
    "    :param neighbors: function which returns the neighbors of a given point.\n",
    "    :type neighbors: list of numpy.array.\n",
    "    :param theta0: initial guess.\n",
    "    :type theta0: numpy.array.\n",
    "    :param epsilon: used to stop the optimization if the current cost is less than epsilon.\n",
    "    :type epsilon: float.\n",
    "    :param max_iterations: maximum number of iterations.\n",
    "    :type max_iterations: int.\n",
    "    :return theta: local minimum.\n",
    "    :rtype theta: numpy.array.\n",
    "    :return history: history of points visited by the algorithm.\n",
    "    :rtype history: list of numpy.array.\n",
    "    \"\"\"\n",
    "    theta = theta0\n",
    "    history = [theta0]\n",
    "    # Todo: Implement Hill Climbing\n",
    "    return theta, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GW1r90dOMDfz"
   },
   "source": [
    "# Definindo funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oK_DufB3ntzC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from math import pi, cos, sin\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_learning_curve(history):\n",
    "    \"\"\"\n",
    "    Plots the learning curve.\n",
    "\n",
    "    :param history: points visited by the optimization algorithm.\n",
    "    :type history: list of numpy.array.\n",
    "    \"\"\"\n",
    "    costs = [cost_function(theta) for theta in history]\n",
    "    plt.plot(costs)\n",
    "    plt.grid()\n",
    "\n",
    "\n",
    "def plot_optimization(history):\n",
    "    \"\"\"\n",
    "    Plots the optimization history.\n",
    "\n",
    "    :param history: points visited by the optimization algorithm.\n",
    "    :type history: list of numpy.array.\n",
    "    \"\"\"\n",
    "    t0 = np.arange(-0.5, 0.5, 0.01)\n",
    "    t1 = np.arange(-0.5, 0.5, 0.01)\n",
    "    z = np.zeros((len(t0), len(t1)))\n",
    "    for i in range(len(t0)):\n",
    "        for j in range(len(t1)):\n",
    "            z[i, j] = cost_function(np.array([t0[i], t1[j]]))\n",
    "    plt.contourf(t0, t1, z.transpose())\n",
    "    hx = []\n",
    "    hy = []\n",
    "    for h in history:\n",
    "        hx.append(h[0])\n",
    "        hy.append(h[1])\n",
    "    plt.xlabel(\"v_0 (m/s)\")\n",
    "    plt.ylabel(\"f (m/s^2)\")\n",
    "    (handle,) = plt.plot(hx, hy, \".-\", markersize=5)\n",
    "    plt.plot(hx[0], hy[0], \"*y\")\n",
    "    plt.plot(hx[-1], hy[-1], \"xr\")\n",
    "    return handle\n",
    "\n",
    "\n",
    "def cost_function(theta):\n",
    "    \"\"\"\n",
    "    Samples the linear regression cost function.\n",
    "\n",
    "    :param theta: parameter point.\n",
    "    :type theta: numpy.array.\n",
    "    :return: cost value at theta.\n",
    "    :rtype: float.\n",
    "    \"\"\"\n",
    "    return sum((theta[0] + theta[1] * t - v) ** 2) / (2.0 * m)\n",
    "\n",
    "\n",
    "def gradient_function(theta):\n",
    "    \"\"\"\n",
    "    Samples the gradient of the linear regression cost function.\n",
    "\n",
    "    :param theta: parameter point.\n",
    "    :type theta: numpy.array.\n",
    "    :return: gradient at theta.\n",
    "    :rtype: float.\n",
    "    \"\"\"\n",
    "    global t, v, m\n",
    "    return np.array(\n",
    "        [\n",
    "            (1 / m) * sum(theta[0] + theta[1] * t - v),\n",
    "            (1 / m) * sum((theta[0] + theta[1] * t - v) * t),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_least_squares():\n",
    "    \"\"\"\n",
    "    Uses the Least Squares Method to fit the ball parameters.\n",
    "\n",
    "    :return: array containing the initial speed and the acceleration factor due to rolling friction.\n",
    "    :rtype: numpy.array.\n",
    "    \"\"\"\n",
    "    global t, v\n",
    "    return least_squares([lambda x: 1.0, lambda x: x], t, v)\n",
    "\n",
    "\n",
    "def fit_gradient_descent():\n",
    "    \"\"\"\n",
    "    Uses Gradient Descent (GD) to fit the ball parameters.\n",
    "\n",
    "    :return theta: array containing the initial speed and the acceleration factor due to rolling friction.\n",
    "    :rtype theta: numpy.array.\n",
    "    :return history: history of points visited by the algorithm.\n",
    "    :rtype history: list of numpy.array.\n",
    "    \"\"\"\n",
    "    theta, history = gradient_descent(\n",
    "        cost_function, gradient_function, np.array([0.0, 0.0]), 0.1, 1.0e-10, 1000\n",
    "    )\n",
    "    return theta, history\n",
    "\n",
    "\n",
    "def fit_hill_climbing():\n",
    "    \"\"\"\n",
    "    Uses Hill Climbing (HC) to fit the ball parameters.\n",
    "\n",
    "    :return theta: array containing the initial speed and the acceleration factor due to rolling friction.\n",
    "    :rtype theta: numpy.array.\n",
    "    :return history: history of points visited by the algorithm.\n",
    "    :rtype history: list of numpy.array.\n",
    "    \"\"\"\n",
    "    delta = 2.0e-3\n",
    "    num_neighbors = 8\n",
    "\n",
    "    def neighbors(theta):\n",
    "        \"\"\"\n",
    "        Returns 8-connected neighbors of point theta.\n",
    "        The neighbors are sampled around a circle of radius \"delta\".\n",
    "        Equally spaced (in terms of angle) \"num_neighbors\" neighbors are sampled.\n",
    "\n",
    "        :param theta: current point.\n",
    "        :type theta: numpy.array.\n",
    "        :return: neighbors of theta.\n",
    "        :rtype: list of numpy.array.\n",
    "        \"\"\"\n",
    "        neigh = []\n",
    "        delta_angle = 2 * pi / num_neighbors\n",
    "        for a in range(num_neighbors):\n",
    "            neigh.append(\n",
    "                np.array(\n",
    "                    [\n",
    "                        theta[0] + delta * cos(a * delta_angle),\n",
    "                        theta[1] + delta * sin(a * delta_angle),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        return neigh\n",
    "\n",
    "    theta, history = hill_climbing(\n",
    "        cost_function, neighbors, np.array([0.0, 0.0]), 1.0e-10, 1000\n",
    "    )\n",
    "    return theta, history\n",
    "\n",
    "\n",
    "def fit_simulated_annealing():\n",
    "    \"\"\"\n",
    "    Uses Simulated Annealing (SA) to fit the ball parameters.\n",
    "\n",
    "    :return theta: array containing the initial speed and the acceleration factor due to rolling friction.\n",
    "    :rtype theta: numpy.array.\n",
    "    :return history: history of points visited by the algorithm.\n",
    "    :rtype history: list of numpy.array.\n",
    "    \"\"\"\n",
    "    temperature0 = 1.0\n",
    "    beta = 1.0\n",
    "    delta = 2.0e-3\n",
    "\n",
    "    def random_neighbor(theta):\n",
    "        \"\"\"\n",
    "        Returns a random neighbor of theta.\n",
    "        The random neighbor is sampled around a circle of radius <delta>.\n",
    "        The probability distribution of the angle is uniform(-pi, pi).\n",
    "\n",
    "        :param theta: current point.\n",
    "        :type theta: numpy.array.\n",
    "        :return: random neighbor.\n",
    "        :rtype: numpy.array.\n",
    "        \"\"\"\n",
    "        direction = random.uniform(-pi, pi)\n",
    "        return np.array(\n",
    "            [theta[0] + delta * cos(direction), theta[1] + delta * sin(direction)]\n",
    "        )\n",
    "\n",
    "    def schedule(i):\n",
    "        \"\"\"\n",
    "        Defines the temperature schedule of the simulated annealing.\n",
    "\n",
    "        :param i: current iteration.\n",
    "        :type i: int.\n",
    "        :return: current temperature.\n",
    "        :rtype: float.\n",
    "        \"\"\"\n",
    "        return temperature0 / (1.0 + beta * i * i)\n",
    "\n",
    "    theta, history = simulated_annealing(\n",
    "        cost_function, random_neighbor, schedule, np.array([0.0, 0.0]), 1.0e-10, 5000\n",
    "    )\n",
    "    return theta, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJ4jkfUgMJ-5"
   },
   "source": [
    "# Resolvendo o problema de *fit* usando os algoritmos de otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fN7SCGibny4H"
   },
   "outputs": [],
   "source": [
    "fig_format = \"png\"\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "random.seed(100)\n",
    "# Loading and pre-processing data\n",
    "data = np.genfromtxt(\"data.txt\")\n",
    "t = data[:, 0]\n",
    "x = data[:, 1]\n",
    "y = data[:, 2]\n",
    "t -= t[0]\n",
    "m = len(t)\n",
    "vx = np.zeros(m)\n",
    "vy = np.zeros(m)\n",
    "vx[0] = (x[1] - x[0]) / (t[1] - t[0])\n",
    "vy[0] = (y[1] - y[0]) / (t[1] - t[0])\n",
    "for k in range(1, m - 1):\n",
    "    vx[k] = (x[k + 1] - x[k - 1]) / (t[k + 1] - t[k - 1])\n",
    "    vy[k] = (y[k + 1] - y[k - 1]) / (t[k + 1] - t[k - 1])\n",
    "vx[-1] = (x[-1] - x[-2]) / (t[-1] - t[-2])\n",
    "vy[-1] = (y[-1] - y[-2]) / (t[-1] - t[-2])\n",
    "v = np.sqrt(vx ** 2 + vy ** 2)\n",
    "\n",
    "# Solving the problem using Least Squares in order to obtain ground truth\n",
    "theta_ls = fit_least_squares()\n",
    "print(\"Least Squares solution: \", theta_ls)\n",
    "# Solving the problem using each algorithm\n",
    "theta_gd, history_gd = fit_gradient_descent()\n",
    "print(\"Gradient Descent solution: \", theta_gd)\n",
    "theta_hc, history_hc = fit_hill_climbing()\n",
    "print(\"Hill Climbing solution: \", theta_hc)\n",
    "theta_sa, history_sa = fit_simulated_annealing()\n",
    "print(\"Simulated Annealing solution: \", theta_sa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6heQ1zHMSCK"
   },
   "source": [
    "# Traçando a evolução X-Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2KEAa2ytrpU"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_optimization(history_gd)\n",
    "plt.title(\"Gradient Descent\")\n",
    "plt.savefig(\"xy_gradient_descent.%s\" % fig_format, format=fig_format)\n",
    "plt.figure()\n",
    "plot_optimization(history_hc)\n",
    "plt.title(\"Hill Climbing\")\n",
    "plt.savefig(\"xy_hill_climbing.%s\" % fig_format, format=fig_format)\n",
    "plt.figure()\n",
    "plot_optimization(history_sa)\n",
    "plt.title(\"Simulated Annealing\")\n",
    "plt.savefig(\"xy_simulated_annealing.%s\" % fig_format, format=fig_format)\n",
    "\n",
    "# Plotting the optimization histories in a single plot for comparison\n",
    "plt.figure()\n",
    "handle_gd = plot_optimization(history_gd)\n",
    "handle_hc = plot_optimization(history_hc)\n",
    "handle_sa = plot_optimization(history_sa)\n",
    "plt.legend(\n",
    "    [handle_gd, handle_hc, handle_sa],\n",
    "    [\"Gradient Descent\", \"Hill Climbing\", \"Simulated Annealing\"],\n",
    ")\n",
    "plt.title(\"Optimization Comparison\")\n",
    "plt.savefig(\"xy_comparison.%s\" % fig_format, format=fig_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdMoyKeSMbF2"
   },
   "source": [
    "# Traçando as curvas de aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7e5GX5KuCnQ"
   },
   "outputs": [],
   "source": [
    "# Plotting learning curves\n",
    "plt.figure()\n",
    "plot_learning_curve(history_gd)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Gradient Descent\")\n",
    "plt.savefig(\"learning_gradient_descent.%s\" % fig_format, format=fig_format)\n",
    "plt.figure()\n",
    "plot_learning_curve(history_hc)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Hill Climbing\")\n",
    "plt.savefig(\"learning_hill_climbing.%s\" % fig_format, format=fig_format)\n",
    "plt.figure()\n",
    "plot_learning_curve(history_sa)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Simulated Annealing\")\n",
    "plt.savefig(\"learning_simulated_annealing.%s\" % fig_format, format=fig_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7L9hhjWRMeNI"
   },
   "source": [
    "# Traçando a curva ajustada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w64wlsC4uGT2"
   },
   "outputs": [],
   "source": [
    "# Plotting the curve fit\n",
    "plt.figure()\n",
    "plt.plot(t, v, \"*k\")\n",
    "v_ls = theta_ls[0] + theta_ls[1] * t\n",
    "v_gd = theta_gd[0] + theta_gd[1] * t\n",
    "v_hc = theta_hc[0] + theta_hc[1] * t\n",
    "v_sa = theta_sa[0] + theta_sa[1] * t\n",
    "plt.plot(t, v_ls, \"tab:red\")\n",
    "plt.plot(t, v_gd, \"tab:blue\")\n",
    "plt.plot(t, v_hc, \"tab:orange\")\n",
    "plt.plot(t, v_sa, \"tab:green\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Speed (m/s)\")\n",
    "plt.legend(\n",
    "    [\n",
    "        \"Data Points\",\n",
    "        \"Least Squares\",\n",
    "        \"Gradient Descent\",\n",
    "        \"Hill Climbing\",\n",
    "        \"Simulated Annealing\",\n",
    "    ]\n",
    ")\n",
    "plt.title(\"Curve Fit\")\n",
    "plt.savefig(\"fit_comparison.%s\" % fig_format, format=fig_format)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "402ed06ea01c8c1a55b828d57c6c3919ad30925f1c095ebcbd3f33910cca0486"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
